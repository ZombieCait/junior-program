{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План:\n",
    "1. Python\n",
    "2. Numpy\n",
    "3. Pandas\n",
    "4. Regexp + regexp with dataframes\n",
    "5. <b>Texts</b>\n",
    "6. Matplotlib, Seaborn\n",
    "7. Classification, clustering, binary, multiclass, multilabel\n",
    "8. Metrics\n",
    "9. Feature extraction\n",
    "10. Pyspark/sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXTS\n",
    "\n",
    "Рассмотрим:\n",
    "\n",
    "* предобработку текста\n",
    "* представление текста\n",
    "* понятие эмбеддинга\n",
    "* текстовую классификацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://media.giphy.com/media/nopqz91prOyvS/giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предобработка текста\n",
    "\n",
    "Текст на естественном языке, который нужно обрабатывать в задачах машинного обучения, сильно зависит от источника. Пример:\n",
    "\n",
    "Википедия\n",
    "> Литературный язык — обработанная часть общенародного языка, обладающая в большей или меньшей степени письменно закреплёнными нормами; язык всех проявлений культуры, выражающихся в словесной форме.\n",
    "\n",
    "Твиттер\n",
    "> Если у вас в компании есть люди, которые целый день сидят в чатиках и смотрят видосики, то, скорее всего, это ДАТАСАЕНТИСТЫ и у них ОБУЧАЕТСЯ\n",
    "\n",
    "Ответы@Mail.ru\n",
    "> как пишется \"Вообщем лето было отличное\" раздельно или слитно слово ВОобщем?? ?\n",
    "\n",
    "В связи с этим, возникает задача предобработки (или нормализации) текста, то есть приведения к некоторому единому виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Приведение текста к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'купил таблетки от тупости, но не смог открыть банку,ЧТО ДЕЛАТЬ???'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Удаление неинформативных символов.\n",
    "\n",
    "Такими символами могут быть символы пунктуации, спец-символы, повторяющиеся символы, цифры.\n",
    "Удалите символы пунктуации и лишние пробелы из предыдущего текста в нижнем регистре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Разбиение текста на смысловые единицы (токенизация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Купите кружку-термос \"Hello Kitty\" на 0.5л (64см³) за 300 рублей. До 01.01.2020.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к токенизации - это разбиение по текста по пробельным символам. \n",
    "\n",
    "**Quiz: Какая у этого подхода есть проблема?**\n",
    "\n",
    "- Объем словаря будет большим. В текстах могут встретится как слова **кружка**, **термос** по отдельности так и через тире **кружка-термос** - и при использовании простых способов векторизации текстов это будут 3 разных смысловых единицы, хотя логично было бы оставить 2.\n",
    "\n",
    "- Если перед нами стоит задача, в которой необходимо учитывать пунктуацию, то нам придется придумывать способ оторвать точки, кавычки, скобки от слов\n",
    "    \n",
    "- Если пробелы пропущены, то два слова могут остаться склееными через запятую\\точку\\дефис"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для морфологического анализа для русского языка [`pymorphy2`](https://pymorphy2.readthedocs.io/en/latest/) есть простая вспомогательная функция для токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложной метод токенизации представлен в [`nltk`](https://www.nltk.org/): библиотеке для общего NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сравните и напишите в комментарии чем отличаются эти три метода**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка также есть новая специализированная библиотека [`razdel`](https://github.com/natasha/razdel).\n",
    "\n",
    "**Напишите функцию, которая принимает на вход текст и возвращает список токенов из метода tokenize библиотеки razdel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "\n",
    "def tokenize_with_razdel(text):\n",
    "    return ##your code\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Приведение слов к нормальной форме (стемминг, лемматизация)\n",
    "\n",
    "**Стемминг - это нормализация слова путём отбрасывания окончания по правилам языка.**\n",
    "\n",
    "Такая нормализация хорошо подходит для языков с небольшим разнообразием словоформ, например, для английского. В библиотеке nltk есть несколько реализаций стеммеров:\n",
    " - Porter stemmer\n",
    " - Snowball stemmer - только его можно использовать для русского языка (но лучше не надо)\n",
    " - Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка этот подход не очень подходит, поскольку в русском есть падежные формы, время у глаголов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежа'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer(language='russian').stem('бежать')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две самые часто используемые библиотеки для лемматизации русских слов:\n",
    "- [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "- [mystem3](https://tech.yandex.ru/mystem/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к лемматизации - словарный. Здесь не учитывается контекст слова, поэтому для омонимов такой подход работает не всегда. Такой подход применяет библиотека pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напишите функцию принимающую на вход список токенов и возвращаюшую список лемматизированных с помощью pymorphy токенов**\n",
    "\n",
    "протестируйте на примере 'на заводе стали увидел виды стали'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_pymorphy(tokens):\n",
    "    return  ##your code\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека от Яндекса `mystem3` обходит это ограничение и рассматривает контекст слова, используя статистику и правила. + Имеет свой токенизатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напишите функцию принимающую на вход строку и возвращаюшую список лемматизированных токенов с помощью pymystem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def lemmatize_with_mystem(text):\n",
    "    ##your code\n",
    "    \n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще более крутая и более **медленная** библиотека `RNNMorph` базирующаяся на рекурентных сетях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "\n",
    "def lemmatize_with_rnnmorph(tokens):\n",
    "    #you know what to do\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "<img src=\"gifs/one_hot.png\" width=\"500\">\n",
    "\n",
    "При таком способе представления текстов, составляется словарь всех слов со всех документов - столбцы нашей матрицы, строки это документы. \n",
    "\n",
    "Если слово есть в документе на пересечении столбца и строки ставится 1, иначе 0. \n",
    "\n",
    "Матрица сформированная таким образом получается разреженной, т.е. если у нас очень много уникальных слов, доля единиц в строке будет маленькой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы уже получали one-hot вектора с помощью pandas\\numpy. \n",
    "Сначала нам нужно каждому слову поставить в соответствие номер, а затем перевести их в бинарные вектора. \n",
    "\n",
    "В этот раз используем библиотеку scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "words = ['What', 'the', 'hell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получите one-hot вектора используя LabelEncoder и OneHotEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что будет, если мы сложим все one-hot вектора слов в тексте?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag-of-words\n",
    "\n",
    "В прошлом методе, если слово употребляется в тексте на пересечении столбца-слова и строки-документа ставились 1, но теперь мы бы хотели так же знать сколько раз слово встретилось в данном документе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы посчитать количество слов в тексте, используем метод CountVectorizer из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Кот пьет молоко',\n",
    "    'Кто пьет молоко?',\n",
    "    'Молоко выпивается котом',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучите CountVectorizer на примерах, выведите вектора для трех предложений и список слов-столбцов матрицы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF\n",
    "\n",
    "**Term Frequency**  $tf(w,d)$ - сколько раз слово $w$ встретилось в документе $d$\n",
    "\n",
    "**Document Frequency** $df(w)$ - сколько документов содержат слово $w$\n",
    "\n",
    "**Inverse Document Frequency** $idf(w) = log_2(N/df(w))$  — обратная документная частотность. \n",
    "\n",
    "**TF-IDF**=$tf(w,d)*idf(w)$\n",
    "\n",
    "В гите есть презентация, с которой возможно, станет понятнее **`векторайзеры и метрики.pptx`**\n",
    "\n",
    "**Обучите TfidfVectorizer на примерах из предыдущего задания, выведите вектора для трех предложений и список слов-столбцов матрицы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы попробуем применить описание методы предобработки и представления текста на примере анализа тональности текста. В качестве данных будем использовать небольшой датасет твитов. Всего в данных 2 класса: позитив и негатив."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Загрузка данных и получение тренировочной и тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6929, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>эти розы для прекрасной мамочки)))=_=]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>И да, у меня в этом году серьезные проблемы со...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>♥Обожаю людей, которые заставляют меня смеятьс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Вчера нашла в почтовом ящике пустую упаковку и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>очень долгожданный и хороший день был)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  positive            эти розы для прекрасной мамочки)))=_=]]\n",
       "1  negative  И да, у меня в этом году серьезные проблемы со...\n",
       "2  positive  ♥Обожаю людей, которые заставляют меня смеятьс...\n",
       "3  negative  Вчера нашла в почтовом ящике пустую упаковку и...\n",
       "4  positive             очень долгожданный и хороший день был)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разбейте выборку на две тренировочную и валидационную с помощью функции train_test_split из sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверьте что доли классов на train и на test совпадают**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша выборка не сбалансирована (доля одно из класса значительно ниже доли другого), поэтому стандартные метрики качества для классификаторов вроде accuracy или roc auc нам не подходят"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужна Точность (Precision) и Полнота (Recall)!\n",
    "\n",
    "**Про эти метрики можно подробно почитать в презентации  `векторайзеры и метрики.pptx`**\n",
    "\n",
    "Для подсчета метрик будем использовать говоторые функции из sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки векторизатора. В качестве модели будем использовать линейный SVM, он хорошо работает на задачах классификации текстов, когда для векторизации используются методы дающие разреженные матрицы.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализуйте функцию следуя инструкциям в комментариях**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_vectorizer(vectorizer):\n",
    "    #обучение выбранного векторайзера и получение веторов текстов (fit_transform)  \n",
    "    \n",
    "    #иницилизация классификатора и тренировка модели\n",
    "    \n",
    "    \n",
    "    #получение векторов текстов для теста\n",
    "    \n",
    "    #получение предсказания\n",
    "    \n",
    "    #вывод метрик классификации с помощью функции classification_report - выводит precision и recall для каждого класса\n",
    "    \n",
    "    #возвращаем предсказанные классы для теста\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Сравнение способов представления текста\n",
    "\n",
    "* необработанный текст + используйте CountVectorizer\n",
    "* переведите в lower и используйте CountVectorizer\n",
    "* lower + lemmatization + CountVectorizer\n",
    "* lower + stemming (SnowballStemer) + CountVectorizer\n",
    "Выберите лучший из двух последних ->\n",
    "* lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "* lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel) + передайте список стоп слов (data/stopwords-ru.txt)\n",
    "* lower + lemmatization\\stemming + TfidfVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "* lower + lemmatization\\stemming + TfidfVectorizer (используйте ngrams(2, 2)) + передавайте в векторайзер свой токенизатор (например из razdel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
